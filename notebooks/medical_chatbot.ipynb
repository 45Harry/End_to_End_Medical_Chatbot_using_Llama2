{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216c8b24",
   "metadata": {},
   "source": [
    "# MEDICAL CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b784fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQAWithSourcesChain, create_retrieval_chain\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from llama_cpp import Llama\n",
    "from langchain.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0438095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PINECONE_API_KEY'] ='pcsk_5pg9q6_LDAcg5pvyr5htitdzjFdo6QaQDbbaJWWhs1XCPdVHwQPMcCW3hP7RdeEDztjsp9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7197a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from pdfs\n",
    "def load_pdf (data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob='*.pdf',\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b77d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m extracted_data = \u001b[43mload_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mload_pdf\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_pdf\u001b[39m (data):\n\u001b[32m      3\u001b[39m     loader = DirectoryLoader(data,\n\u001b[32m      4\u001b[39m                     glob=\u001b[33m'\u001b[39m\u001b[33m*.pdf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m                     loader_cls=PyPDFLoader)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     documents = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/langchain_community/document_loaders/directory.py:117\u001b[39m, in \u001b[36mDirectoryLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Document]:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/langchain_community/document_loaders/directory.py:195\u001b[39m, in \u001b[36mDirectoryLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lazy_load_file(i, p, pbar)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[32m    198\u001b[39m     pbar.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/langchain_community/document_loaders/directory.py:223\u001b[39m, in \u001b[36mDirectoryLoader._lazy_load_file\u001b[39m\u001b[34m(self, item, path, pbar)\u001b[39m\n\u001b[32m    221\u001b[39m loader = \u001b[38;5;28mself\u001b[39m.loader_cls(\u001b[38;5;28mstr\u001b[39m(item), **\u001b[38;5;28mself\u001b[39m.loader_kwargs)\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/langchain_community/document_loaders/pdf.py:305\u001b[39m, in \u001b[36mPyPDFLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    304\u001b[39m     blob = Blob.from_path(\u001b[38;5;28mself\u001b[39m.file_path)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parser.lazy_parse(blob)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/langchain_community/document_loaders/parsers/pdf.py:396\u001b[39m, in \u001b[36mPyPDFParser.lazy_parse\u001b[39m\u001b[34m(self, blob)\u001b[39m\n\u001b[32m    394\u001b[39m single_texts = []\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page_number, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pdf_reader.pages):\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m     text_from_page = \u001b[43m_extract_text_from_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m     images_from_page = \u001b[38;5;28mself\u001b[39m.extract_images_from_page(page)\n\u001b[32m    398\u001b[39m     all_text = _merge_text_and_extras(\n\u001b[32m    399\u001b[39m         [images_from_page], text_from_page\n\u001b[32m    400\u001b[39m     ).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/langchain_community/document_loaders/parsers/pdf.py:378\u001b[39m, in \u001b[36mPyPDFParser.lazy_parse.<locals>._extract_text_from_page\u001b[39m\u001b[34m(page)\u001b[39m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m page.extract_text()\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextraction_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextraction_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextraction_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/pypdf/_page.py:2350\u001b[39m, in \u001b[36mPageObject.extract_text\u001b[39m\u001b[34m(self, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text, extraction_mode, *args, **kwargs)\u001b[39m\n\u001b[32m   2347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orientations, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m   2348\u001b[39m     orientations = (orientations,)\n\u001b[32m-> \u001b[39m\u001b[32m2350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2351\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2352\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2353\u001b[39m \u001b[43m    \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCONTENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2359\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/pypdf/_page.py:2117\u001b[39m, in \u001b[36mPageObject._extract_text\u001b[39m\u001b[34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[39m\n\u001b[32m   2115\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2116\u001b[39m         text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2117\u001b[39m         memo_cm = \u001b[43mcm_matrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2118\u001b[39m         memo_tm = tm_matrix.copy()\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "extracted_data = load_pdf('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672072ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'creator': 'PyPDF', 'creationdate': '2004-12-18T17:36:01-05:00', 'moddate': '2004-12-18T17:38:34-06:00', 'source': '../data/Gale Encyclopedia of Medicine Vol. 3 (G-M).pdf', 'total_pages': 941, 'page': 0, 'page_label': '1'}, page_content='The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36def00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 500,chunk_overlap = 20\n",
    "    )\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cf449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37368"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4770ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embedding modle\n",
    "def download_hugging_face_embedding():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8a220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b81c496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7dfb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize the Pinecone client\n",
    "pinecone_client = Pinecone(api_key=\"pcsk_5pg9q6_LDAcg5pvyr5htitdzjFdo6QaQDbbaJWWhs1XCPdVHwQPMcCW3hP7RdeEDztjsp9\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49d4567",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore.from_existing_index(\n",
    "    index_name='medical-chatbot', \n",
    "    embedding=embeddings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4904710",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m index_name = \u001b[33m\"\u001b[39m\u001b[33mmedical-chatbot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m docsearch = PineconeVectorStore.from_texts(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     [t.page_content \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtext_chunks\u001b[49m]\n\u001b[32m      5\u001b[39m     ,embeddings,\n\u001b[32m      6\u001b[39m     index_name=index_name,\n\u001b[32m      7\u001b[39m                             )\n",
      "\u001b[31mNameError\u001b[39m: name 'text_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "docsearch = PineconeVectorStore.from_texts(\n",
    "    [t.page_content for t in text_chunks]\n",
    "    ,embeddings,\n",
    "    index_name=index_name,\n",
    "                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "518bdf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7f4305a87b60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec99e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Use the following pices of information to answer the user's questioin>\n",
    "If you don't know the answer, just say that you don't konw, don't try to make up an anser.\n",
    "\n",
    "Context: {context}\n",
    "Question:{input}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff7bbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,input_variables=['context','input'],\n",
    ")\n",
    "chain_type_kwargs = {'prompt':prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9288059b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../model/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 3615.73 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model: compute buffer total size =   71.84 MB\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(model_path='../model/llama-2-7b-chat.ggmlv3.q4_0.bin',)\n",
    "                   # model_type = 'llama',\n",
    "                    #config={'max_new_tokens':512,\n",
    "                         #   'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333af138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4920739232\n"
     ]
    }
   ],
   "source": [
    "print(os.path.getsize(model_path))  # Expected: ~5-7GB for 8B Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148ecd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\n",
      "error loading model: unknown (magic, version) combination: 46554747, 00000003; is this really a GGML file?\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: ../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf. Received error  [type=value_error, input_value={'model_path': '../model/...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaCpp\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m llm = \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m                    \u001b[38;5;66;03m# model_type = 'llama',\u001b[39;00m\n\u001b[32m      4\u001b[39m                     \u001b[38;5;66;03m#config={'max_new_tokens':512,\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m                          \u001b[38;5;66;03m#   'temperature':0.8})\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: ../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf. Received error  [type=value_error, input_value={'model_path': '../model/...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "llm = LlamaCpp(model_path='../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf',)\n",
    "                   # model_type = 'llama',\n",
    "                    #config={'max_new_tokens':512,\n",
    "                    \n",
    "                         #   'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd4e7f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf: data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!file ../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1ca1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\n",
      "error loading model: unknown (magic, version) combination: 46554747, 00000003; is this really a GGML file?\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[32m      2\u001b[39m model_path=\u001b[33m'\u001b[39m\u001b[33m../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m llm = \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Context window\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to 0 if CPU-only\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(llm.create_completion(\u001b[33m\"\u001b[39m\u001b[33mHello!\u001b[39m\u001b[33m\"\u001b[39m, max_tokens=\u001b[32m512\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/Data_Science/GenerativeAI/.generativeAI/lib/python3.13/site-packages/llama_cpp/llama.py:328\u001b[39m, in \u001b[36mLlama.__init__\u001b[39m\u001b[34m(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr():\n\u001b[32m    325\u001b[39m         \u001b[38;5;28mself\u001b[39m.model = llama_cpp.llama_load_model_from_file(\n\u001b[32m    326\u001b[39m             \u001b[38;5;28mself\u001b[39m.model_path.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m.params\n\u001b[32m    327\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28mself\u001b[39m.ctx = llama_cpp.llama_new_context_with_model(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.params)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model_path='../model/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf'\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,  # Context window\n",
    "    n_gpu_layers=40,  # Set to 0 if CPU-only\n",
    "    verbose=True,\n",
    ")\n",
    "print(llm.create_completion(\"Hello!\", max_tokens=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f1fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = create_retrieval_chain(\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k':2}),\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm = llm,prompt = prompt),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccbeca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result :  Acne is a skin disorder in which the sebaceous glands become inflamed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3810.47 ms\n",
      "llama_print_timings:      sample time =     7.29 ms /    21 runs   (    0.35 ms per token,  2879.47 tokens per second)\n",
      "llama_print_timings: prompt eval time = 89363.06 ms /   271 tokens (  329.75 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:        eval time =  9933.09 ms /    20 runs   (  496.65 ms per token,     2.01 tokens per second)\n",
      "llama_print_timings:       total time = 99598.80 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_input= input(f\"Input Prompt:\")\n",
    "result = qa.invoke({\"input\":user_input})\n",
    "print(\"Question :\",result['input'])\n",
    "print(\"Result : \",result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a888c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is acne?',\n",
       " 'context': [Document(id='e295c9c4-3ce6-44d7-9e0f-a0cf0cce08b5', metadata={}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       "  Document(id='330b134e-6c77-4fd9-991d-8f79ad056305', metadata={}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a womanâ€™s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25')],\n",
       " 'answer': 'Acne is a skin disorder in which the sebaceous glands become inflamed.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0f4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".generativeAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
